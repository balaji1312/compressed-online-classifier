# -*- coding: utf-8 -*-
"""Project Part II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V1JG0jS5ns32XYoPUhtSvUZL7yMvxozQ
"""

pip install cvxpy

pip install cvxopt

import numpy as np
import cvxpy as cp
from sklearn.utils import shuffle
from keras.datasets import mnist
from scipy.spatial import distance

class MyClassifier:
    def __init__(self,M, bound=0.02, lb = 0.001):
        self.M = M #number of features
        self.W=[] #weight
        self.w=[] #bias
        self.ref = [] #stores the ratio of distance between classes
        self.bound = bound #hyperparameter for upper bound on ratio of distances
        self.lb = lb #hyperparameter for lower bound on ratio of distances
        self.labels=np.zeros((2,1)) #labelpair for mnist/synthetic classification
        self.imp_points = [] #maintains a list of all points selected to compute the classifier
        self.len_data = 0 #used to find number of data points used 
    
    def f(self, input):
        y_flat = np.asarray(input).reshape(-1,self.M) #flattens set of images to a 2D array
        decisions = np.sign(y_flat.dot(self.W) + self.w) #The sign function returns -1 if content < 0, 0 if content==0, 1 if content > 0
        return decisions
    
    def ILP(self,selected_sample, selected_label):
        N = selected_sample.shape[0] #total number of samples
        class1 = []
        class2 = []

        trains = selected_sample.reshape(N,-1) #flattens image to vector

        #for loop to separate two classes of samples
        for i in range(N):
            if selected_label[i] == -1:
                class2.append(trains[i])
            else:
                class1.append(trains[i])
        
        #to calculate the mahalanobis distance, we need the centroid and cov matrix of both classes
        c1 = np.array(class1)
        class1_mean = c1.mean(axis=0)
        class1_cov = np.cov(c1.transpose())
        class1_inv = np.linalg.pinv(class1_cov)
        c2 = np.array(class2)
        class2_mean = c2.mean(axis=0)
        class2_cov = np.cov(c2.transpose())
        class2_inv = np.linalg.pinv(class2_cov)

        #ref is used to maintain a ratio of differential distance between correct class and other class 
        ref = np.zeros((N,1))
        #compute the mahalanobis distance for the given point from the two classes
        for i in range(N):
            d1 = distance.mahalanobis(trains[i].flatten(), class1_mean, class1_inv)
            d2 = distance.mahalanobis(trains[i].flatten(), class2_mean, class2_inv)
            if selected_label[i] == 1:
                ref[i] = (d2-d1)/d1
            else:
                ref[i] = (d1-d2)/d2
        
        self.ref = ref

        #Here we define the parameters to solve for the ILP  
        #selection 1 and selection 2 ensure the lower and upper bounds on the values of ref are met
        #while selection ensure both of the above conditions are met at the same time
        selection1 = cp.Variable(N, boolean = True)
        selection2 = cp.Variable(N, boolean = True)
        selection = cp.Variable(N, boolean = True)
        ones_vec = np.ones(N)

        constraints = []
        for i in range(N):
            constraints.append((selection1[i]-0.5) * (ref[i]-self.bound) <= 0)
            constraints.append((selection2[i]-0.5) * (ref[i]+self.lb) >= 0)
            constraints.append(-selection[i]+ selection1[i]+selection2[i] <= 1.5)
            #reg = cp.norm(W, 1)

        #objective function is to minimise the number of points selected
        total_points = ones_vec @ selection

        problem = cp.Problem(cp.Minimize(total_points), constraints)

        #we use mixed integer solver for ease; but the code also runs fine if we solve the LP and round to nearest integer
        problem.solve(solver=cp.GLPK_MI)

        selected_list = selection.value
        print(len(selected_list),sum(selected_list))
        selected_list = selected_list.astype(bool)
        sorted_samples = trains[selected_list]
        sorted_labels = selected_label[selected_list]
        return sorted_samples, sorted_labels
      
    def LP (self, samples,labels):
        y_train = np.asarray(samples).reshape(-1,self.M)
        s_train = np.asarray(labels)
        N_train = y_train.shape[0]
        s_train = np.expand_dims(s_train,axis=1)
        Wx=np.zeros((self.M,1))
        wx=np.zeros((1,1))

        Wx = cp.Variable((self.M,1))
        wx = cp.Variable()
        loss = cp.sum(cp.pos(1 - cp.multiply(s_train, y_train  @ Wx - wx)))
        reg = cp.norm(Wx, 1)
        prob = cp.Problem(cp.Minimize(loss/N_train))

        prob.solve()
        return Wx.value,wx.value

    def train(self, train_data, train_label, l=1e-2, labelpair=[1,7]):

        #First we select only the two classes we are testing on the in the train set, and we replace their corresponding labels with 1,-1
        self.labels = labelpair
        selection_index = np.where((train_label==labelpair[0])|(train_label==labelpair[1]))
        selected_sample = train_data[selection_index]
        selected_label = train_label[selection_index]
        selected_label = np.where(selected_label == labelpair[0], 1, selected_label) #change first label to +1, keep the second label untouched
        selected_label = np.where(selected_label == labelpair[1], -1, selected_label) #change second label to -1

        self.len_data =len(selected_label) #total length of data used to train the classifier
        self.lambd = l

        imp_samples, imp_labels = self.ILP(selected_sample, selected_label)

        self.W, self.w = self.LP(imp_samples, imp_labels)

        used = len(imp_labels)/self.len_data #checks percent of input points used to calculate the classifier parameters
        return used

    def test(self, input):
        decisions = self.f(input)
        #we need to replace the -1,1 values with the actual label for the values of the classes
        decisions = np.where(decisions == 1, self.labels[0], decisions)
        decisions = np.where(decisions == -1, self.labels[1], decisions)
        return decisions
    def getW(self):
        return self.W, self.w

#load data from mnist dataset from keras
(x_train, y_train), (x_test2, y_test2) = mnist.load_data() 
x_train,y_train = shuffle(x_train,y_train) #shuffles train data around

#function to select only the label pairs we desire in the test set 
def selection(training_sample,training_label,labelpair=[1,7]): 

    selection_index = np.where((training_label==labelpair[0])|(training_label==labelpair[1]))
    selected_sample = training_sample[selection_index]
    selected_label = training_label[selection_index]
    return selected_sample, selected_label
     
#function that compares predicted labels ith actual labels, and uses a running list of matches to compute total accuracy on test set
def accuracy(actual_class, estimated_class):
    matching = np.zeros(actual_class.shape[0])
    for i in range(actual_class.shape[0]):
        if actual_class[i] == estimated_class[i]:
            matching[i] = 1
        else:
            matching[i] = 0
    accuracy = np.sum(matching)/actual_class.shape[0]
    return accuracy

bound_ar = [0.04,0.06,0.055,0.05]
lb_ar = [0,0.001,0.002]
acc_ar= [] #list of accuracy for diff values of hyperparameter
used_ar = [] #list of datasamples used for diff values of hyperparameter
labels = [1,7] #digits selected form the MNIST database to classification
for i in range(len(bound_ar)):
  for j in range(len(lb_ar)):
    hyperplane = MyClassifier(784, bound=bound_ar[i],lb=lb_ar[j])
    used = hyperplane.train(x_train,y_train,labelpair=labels)
    used_ar.append(used)
    samples_test, classes_test = selection(x_test2,y_test2, labelpair=labels)
    pred = hyperplane.test(samples_test)
    acc = accuracy(classes_test,pred)
    acc_ar.append(acc)
    print(bound_ar[i],lb_ar[j], 100*acc, 100*used)

#Define train and test set (data points and labels) for synthetic dataset
mean_1 = [-1, 1]
mean_2 = [1,-1]
cov = [[1, 0], [0, 1]]

x1 = np.random.multivariate_normal(mean_1, cov, 6000)
x2 = np.random.multivariate_normal(mean_2, cov, 6000)
x3 = np.random.multivariate_normal(mean_1, cov, 1000)
x4 = np.random.multivariate_normal(mean_2, cov, 1000)
y1 = np.ones(x1.shape[0])
y2 = -1*np.ones(x2.shape[0])
y3 = np.ones(x3.shape[0])
y4= -1*np.ones(x4.shape[0])
synthetic_train=np.concatenate((x1,x2),axis=0)
synthetic_test=np.concatenate((x3,x4),axis=0)
label_train=np.concatenate((y1,y2),axis=0)
label_test=np.concatenate((y3,y4),axis=0)

synthetic_train,label_train = shuffle(synthetic_train,label_train)

#All variables in the below cell are similar to the ones used for driving the MNIST case above, only with a prefix "c" added to them
bound_ar = [0.002]
lb_ar = [0]
cacc_ar= [] 
cused_ar = []
labels= [1,-1]
for i in range(len(bound_ar)):
  for j in range(len(lb_ar)):
    binary = MyClassifier(2, bound=bound_ar[i],lb = lb_ar[j])
    cused = binary.train(synthetic_train,label_train, labelpair = labels)
    cused_ar.append(cused)
    csamples_test, cclasses_test = selection(synthetic_test, label_test, labelpair = labels)
    cpred = binary.test(csamples_test)
    cacc = accuracy(cclasses_test,cpred)
    cacc_ar.append(cacc)
    print(bound_ar[i],lb_ar[j], 100*cacc, 100*cused)