# -*- coding: utf-8 -*-
"""Project I.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qJUIekls56HlPwQdxdW5FtvKpfJpf_D1
"""

pip install cvxpy

import numpy as np
import cvxpy as cp
from sklearn.utils import shuffle
from keras.datasets import mnist

class MyClassifier:
    def __init__(self,M, bound=1):
        self.M = M #number of features
        self.W=[] #weight
        self.w=[] #bias
        self.labels=np.zeros((2,1))
        self.imp_points = [] #maintains a list of all points selected to compute the classifier
        self.class1 = [] #used for sample selection function to split points
        self.class2 = []
        self.lambd = 0 
        self.bound = bound  #variable to determine which points are close enough to decision boundary
        self.count=0 #variable to keep track of iterations through sample_selection; used to reduce computation
        self.len_data = 0 #used to find number of data points used 
    
    def f(self, input):
        y_flat = np.asarray(input).reshape(-1,self.M) #flattens set of images to a 2D array
        decisions = np.sign(y_flat.dot(self.W) + self.w) #The sign function returns -1 if content < 0, 0 if content==0, 1 if content > 0
        return decisions

    #sample_selection takes a given sample, and checks the distance between it an the separating hyperplane. If this distance is less that the hyperparameter
    #bound, then we select the point and add it to running list of important points
    def sample_selection(self, training_sample,training_label,labelpair=[1,7]):
      #as recomputing the values for the separating hyperplane (by solving the LP) is expensive, the following if statement does it for only once after 10 points are added to the list
        if self.count%10 ==0:
          y_train = np.asarray(list(x[0] for x in self.imp_points)).reshape(-1,self.M)
          s_train = np.asarray(list(x[1] for x in self.imp_points))
          N_train = y_train.shape[0]
          s_train = np.expand_dims(s_train,axis=1)
          self.W=np.zeros((self.M,1))
          self.w=np.zeros((1,1))

          W = cp.Variable((self.M,1))
          w = cp.Variable()
          loss = cp.sum(cp.pos(1 - cp.multiply(s_train, y_train  @ W - w)))
          reg = cp.norm(W, 1)
          prob = cp.Problem(cp.Minimize(loss/N_train))

          prob.solve()
          self.W=W.value
          self.w=w.value
        
        proj = np.absolute((np.dot(self.W.flatten(),training_sample.flatten())+self.w))/np.linalg.norm(self.W) #compute the dist bw point and classifier
        #decide if point is important, and which class to append to
        if proj<self.bound :
          self.imp_points.append((training_sample,training_label))
          self.count+=1
          if training_label == 1:
            self.class1.append(training_sample)
          else:
            self.class2.append(training_sample)
          
    #use parameter p to determine what proportion of the data to use to initialise the classes
    def train(self, train_data, train_label,p=0.05,l=1e-2,labelpair=[1,7]):

        #First we select only the two classes we are testing on the in the train set, and we replace their corresponding labels with 1,-1
        self.labels = labelpair
        selection_index = np.where((train_label==labelpair[0])|(train_label==labelpair[1]))
        selected_sample = train_data[selection_index]
        selected_label = train_label[selection_index]
        selected_label = np.where(selected_label == labelpair[0], 1, selected_label) #change first label to +1, keep the second label untouched
        selected_label = np.where(selected_label == labelpair[1], -1, selected_label) #change second label to -1

        self.len_data =len(selected_label) #total length of data used to train the classifier
        self.lambd = l
        min_rand = p*self.len_data #number of samples we are selecting initially to get an estimate for the classifier
        for i in range(self.len_data):
          #First min_rand samples are added directly to set of important points
          if i<min_rand:
            self.imp_points.append((selected_sample[i],selected_label[i]))
            if selected_label[i] == 1:
              self.class1.append(selected_sample[i])
            else:
              self.class2.append(selected_sample[i])
            continue

          self.sample_selection(selected_sample[i],selected_label[i],labelpair = self.labels)

        used = len(self.imp_points)/self.len_data #checks percent of input points used to calculate the classifier parameters
        return used

    def test(self, input):
        decisions = self.f(input)
        #we need to replace the -1,1 values with the actual label for the values of the classes
        decisions = np.where(decisions == 1, self.labels[0], decisions)
        decisions = np.where(decisions == -1, self.labels[1], decisions)
        return decisions
    def getW(self):
        return self.W, self.w

#load data from mnist dataset from keras
(x_train, y_train), (x_test2, y_test2) = mnist.load_data() 
x_train,y_train = shuffle(x_train,y_train) #shuffles train data around

#function to select only the label paris we desire in the test set 
def selection(training_sample,training_label,labelpair=[1,7]): 

    selection_index = np.where((training_label==labelpair[0])|(training_label==labelpair[1]))
    selected_sample = training_sample[selection_index]
    selected_label = training_label[selection_index]
    return selected_sample, selected_label
     
#function that compares predicted labels ith actual labels, and uses a running list of matches to compute total accuracy on test set
def accuracy(actual_class, estimated_class):
    matching = np.zeros(actual_class.shape[0])
    for i in range(actual_class.shape[0]):
        if actual_class[i] == estimated_class[i]:
            matching[i] = 1
        else:
            matching[i] = 0
    accuracy = np.sum(matching)/actual_class.shape[0]
    return accuracy

bound_ar = [0.1,0.5,1]
acc_ar= [] #list of accuracy for diff values of hyperparameter
used_ar = [] #list of datasamples used for diff values of hyperparameter
labels = [1,7] #digits selected form the MNIST database to classification
for i in range(len(bound_ar)):
  hyperplane = MyClassifier(784, bound=bound_ar[i])
  used = hyperplane.train(x_train,y_train,p=0.0005, labelpair=labels)
  used_ar.append(used)
  samples_test, classes_test = selection(x_test2,y_test2, labelpair=labels)
  pred = hyperplane.test(samples_test)
  acc = accuracy(classes_test,pred)
  acc_ar.append(acc)
  print(bound_ar[i], 100*acc, 100*used)

#Define train and test set (data points and labels) for synthetic dataset
mean_1 = [-1, 1]
mean_2 = [1,-1]
cov = [[1, 0], [0, 1]]

x1 = np.random.multivariate_normal(mean_1, cov, 6000)
x2 = np.random.multivariate_normal(mean_2, cov, 6000)
x3 = np.random.multivariate_normal(mean_1, cov, 1000)
x4 = np.random.multivariate_normal(mean_2, cov, 1000)
y1 = np.ones(x1.shape[0])
y2 = -1*np.ones(x2.shape[0])
y3 = np.ones(x3.shape[0])
y4= -1*np.ones(x4.shape[0])
synthetic_train=np.concatenate((x1,x2),axis=0)
synthetic_test=np.concatenate((x3,x4),axis=0)
label_train=np.concatenate((y1,y2),axis=0)
label_test=np.concatenate((y3,y4),axis=0)

synthetic_train,label_train = shuffle(synthetic_train,label_train)

#All variables in the below cell are similar to the ones used for driving the MNIST case above, only with a prefix "c" added to them
bound_ar = [0.001,0.0025,0.005]
cacc_ar= [] 
cused_ar = []
labels= [1,-1]
for i in range(len(bound_ar)):
  binary = MyClassifier(2, bound=bound_ar[i])
  cused = binary.train(synthetic_train,label_train,p=0.0005, labelpair = labels)
  cused_ar.append(cused)
  csamples_test, cclasses_test = selection(synthetic_test, label_test, labelpair = labels)
  cpred = binary.test(csamples_test)
  cacc = accuracy(cclasses_test,cpred)
  cacc_ar.append(cacc)
  print(bound_ar[i], 100*cacc, 100*cused)